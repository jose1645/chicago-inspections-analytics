{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook para verificar funcionamiento de script de ingesta; primero verificaremos el trabajo de la ingesta inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesta inicial cargada exitosamente.\n",
      "Fecha más reciente en el DataFrame: 2022-12-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import pickle\n",
    "\n",
    "# Configurar la conexión con S3 usando variables de entorno\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id='AKIAQLSIVK4FE57FJR65',\n",
    "    aws_secret_access_key='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'\n",
    ")\n",
    "\n",
    "bucket_name = 'chicago-inspections-analytics'\n",
    "ingesta_inicial_key = 'ingesta/inicial/inspecciones-historicas-2024-11-12T19-51-57.pkl'  # Ruta del archivo en S3\n",
    "\n",
    "# Función para cargar el archivo de ingesta inicial desde S3\n",
    "def cargar_ingesta_inicial():\n",
    "    try:\n",
    "        # Descargar el archivo de S3\n",
    "        file_obj = s3_client.get_object(Bucket=bucket_name, Key=ingesta_inicial_key)\n",
    "        file_data = file_obj['Body'].read()\n",
    "        \n",
    "        # Convertir el archivo de Pickle a un DataFrame\n",
    "        df = pickle.loads(file_data)\n",
    "        print(\"Ingesta inicial cargada exitosamente.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Error al cargar la ingesta inicial desde S3:\", e)\n",
    "        return None\n",
    "\n",
    "# Cargar el DataFrame de ingesta inicial\n",
    "df_inicial = cargar_ingesta_inicial()\n",
    "\n",
    "# Obtener la fecha más reciente\n",
    "if df_inicial is not None:\n",
    "    # Asegurarse de que la columna de fecha esté en el formato datetime\n",
    "    df_inicial['inspection_date'] = pd.to_datetime(df_inicial['inspection_date'], errors='coerce')\n",
    "\n",
    "    # Obtener la fecha más reciente\n",
    "    fecha_mas_reciente = df_inicial['inspection_date'].max()\n",
    "    print(\"Fecha más reciente en el DataFrame:\", fecha_mas_reciente)\n",
    "else:\n",
    "    print(\"No se pudo cargar el DataFrame de ingesta inicial.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora verificaremos las ingestas secuenciales, tomando de ejemplo un archivo secuencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import pickle\n",
    "\n",
    "# Configurar la conexión con S3 usando variables de entorno para las credenciales\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id='AKIAQLSIVK4FE57FJR65',\n",
    "    aws_secret_access_key='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'\n",
    ")\n",
    "\n",
    "bucket_name = 'chicago-inspections-analytics'\n",
    "prefix = 'ingesta/consecutiva/'  # Prefijo para archivos de ingestas consecutivas\n",
    "\n",
    "# Listar y cargar cada archivo con el prefijo especificado\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            file_key = obj['Key']\n",
    "            print(f\"\\n--- Cargando datos desde: {file_key} ---\")\n",
    "            \n",
    "            # Descargar el archivo desde S3\n",
    "            file_obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "            file_data = file_obj['Body'].read()\n",
    "            \n",
    "            # Cargar los datos del archivo en un DataFrame\n",
    "            df = pickle.loads(file_data)\n",
    "            \n",
    "            # Mostrar la estructura de cada DataFrame individualmente\n",
    "            print(\"Información del DataFrame:\")\n",
    "            print(df.info())\n",
    "            print(\"\\nPrimeras filas del DataFrame:\")\n",
    "            print(df.head())\n",
    "    else:\n",
    "        print(\"No se encontraron archivos en el prefijo especificado.\")\n",
    "except Exception as e:\n",
    "    print(\"Error al listar o cargar los archivos del bucket:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consultaremos el archivo que controla las ingestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import boto3\n",
    "\n",
    "# Configurar la conexión con S3 usando variables de entorno para las credenciales\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id='AKIAQLSIVK4FE57FJR65',\n",
    "    aws_secret_access_key='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'\n",
    ")\n",
    "\n",
    "bucket_name = 'chicago-inspections-analytics'\n",
    "estado_file_name = 'estado/last_processed_date.pkl'  # Ruta del archivo en S3\n",
    "\n",
    "# Función para cargar la última fecha procesada desde el archivo de estado en S3\n",
    "def cargar_fecha_ultimo_proceso():\n",
    "    try:\n",
    "        # Descargar el archivo desde S3\n",
    "        file_obj = s3_client.get_object(Bucket=bucket_name, Key=estado_file_name)\n",
    "        file_data = file_obj['Body'].read()\n",
    "        \n",
    "        # Deserializar el archivo Pickle para obtener la fecha\n",
    "        fecha = pickle.loads(file_data)\n",
    "        print(f\"Última fecha procesada cargada: {fecha}\")\n",
    "        return fecha\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        print(\"Archivo de estado no encontrado en S3.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(\"Error al cargar la fecha desde S3:\", e)\n",
    "        return None\n",
    "\n",
    "# Ejecutar la función y cargar la fecha\n",
    "fecha_ultimo_proceso = cargar_fecha_ultimo_proceso()\n",
    "\n",
    "# Explorar la fecha cargada\n",
    "if fecha_ultimo_proceso is not None:\n",
    "    print(\"Fecha de la última ingesta procesada:\", fecha_ultimo_proceso)\n",
    "else:\n",
    "    print(\"No se pudo cargar la fecha de la última ingesta procesada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el Script que esta cargado en EC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from sodapy import Socrata\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Intervalo de tiempo entre ingestas en segundos\n",
    "interval = 60\n",
    "s3_bucket = os.getenv(\"S3_BUCKET_NAME\")\n",
    "socrata_username = os.getenv(\"SOCRATA_USERNAME\")\n",
    "socrata_password = os.getenv(\"SOCRATA_PASSWORD\")\n",
    "socrata_app_token = os.getenv(\"SOCRATA_APP_TOKEN\")\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "# Archivo de estado para la última fecha procesada en S3\n",
    "estado_file_name = \"estado/last_processed_date.pkl\"\n",
    "\n",
    "# Función para obtener un cliente de la API de Socrata\n",
    "def get_client():\n",
    "    return Socrata('data.cityofchicago.org', socrata_app_token, socrata_username, socrata_password)\n",
    "\n",
    "# Función para crear el recurso de S3\n",
    "def get_s3_resource():\n",
    "    return boto3.resource(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key\n",
    "    )\n",
    "\n",
    "# Función para subir datos al bucket de S3\n",
    "def guardar_ingesta(bucket_name, ruta, data_frame):\n",
    "    s3 = get_s3_resource()\n",
    "    pickle_data = pickle.dumps(data_frame)  # Convertir a Pickle\n",
    "    s3.Object(bucket_name, ruta).put(Body=pickle_data)\n",
    "    print(f\"Datos guardados en {bucket_name}/{ruta}\")\n",
    "\n",
    "# Función para la ingesta inicial, limitando a datos hasta el año 2022\n",
    "def ingesta_inicial(client, limit=300000):\n",
    "    print(\"Realizando la ingesta de datos inicial hasta el año 2022 desde la API...\")\n",
    "    query = \"inspection_date < '2023-01-01T00:00:00'\"\n",
    "    results = client.get(\"4ijn-s7e5\", where=query, limit=limit)\n",
    "    return pd.DataFrame.from_records(results)\n",
    "\n",
    "# Función para la ingesta consecutiva de datos\n",
    "def ingesta_consecutiva(client, fecha_inicio, limit=1000):\n",
    "    print(f\"Realizando la ingesta incremental desde {fecha_inicio}...\")\n",
    "    query = f\"inspection_date > '{fecha_inicio}'\"\n",
    "    results = client.get(\"4ijn-s7e5\", where=query, limit=limit)\n",
    "    return pd.DataFrame.from_records(results)\n",
    "\n",
    "# Función para verificar acceso a S3\n",
    "def verificar_acceso_s3(bucket_name):\n",
    "    try:\n",
    "        s3 = get_s3_resource()\n",
    "        for obj in s3.Bucket(bucket_name).objects.all():\n",
    "            print(f\"Acceso confirmado a S3. Objeto: {obj.key}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error al acceder a S3: {e}\")\n",
    "        return False\n",
    "\n",
    "# Función para cargar la última fecha procesada desde el archivo de estado en S3\n",
    "def cargar_fecha_ultimo_proceso():\n",
    "    s3 = get_s3_resource()\n",
    "    try:\n",
    "        obj = s3.Object(s3_bucket, estado_file_name)\n",
    "        with obj.get()['Body'] as f:\n",
    "            fecha = pickle.load(f)\n",
    "            print(f\"Última fecha procesada cargada: {fecha}\")\n",
    "            return fecha\n",
    "    except s3.meta.client.exceptions.NoSuchKey:\n",
    "        print(\"Archivo de estado no encontrado, iniciando ingesta completa.\")\n",
    "        return None\n",
    "\n",
    "# Función para guardar la última fecha procesada en el archivo de estado en S3\n",
    "def guardar_fecha_ultimo_proceso(fecha):\n",
    "    s3 = get_s3_resource()\n",
    "    pickle_data = pickle.dumps(fecha)\n",
    "    s3.Object(s3_bucket, estado_file_name).put(Body=pickle_data)\n",
    "    print(f\"Última fecha procesada actualizada a {fecha}\")\n",
    "\n",
    "# Función principal para la ingesta y almacenamiento\n",
    "def ingest_data():\n",
    "    client = get_client()\n",
    "    fecha_hoy = datetime.today().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "\n",
    "    # Verificar acceso a S3\n",
    "    if not verificar_acceso_s3(s3_bucket):\n",
    "        print(\"No se pudo acceder a S3. Deteniendo el proceso de ingesta.\")\n",
    "        return\n",
    "\n",
    "    # Verificar si es una ingesta inicial o consecutiva\n",
    "    fecha_ultimo_proceso = cargar_fecha_ultimo_proceso()\n",
    "\n",
    "    if fecha_ultimo_proceso:\n",
    "        # Ingesta consecutiva\n",
    "        new_data = ingesta_consecutiva(client, fecha_ultimo_proceso)\n",
    "        new_data.columns = new_data.columns.str.strip().str.lower()\n",
    "\n",
    "        # Verificar si hay datos nuevos antes de guardar en S3\n",
    "        if not new_data.empty:\n",
    "            # Generar nombre del archivo S3 para la ingesta consecutiva con fecha dinámica en cada archivo\n",
    "            s3_object_name = f\"ingesta/consecutiva/inspecciones-consecutivas-{fecha_hoy}.pkl\"\n",
    "            guardar_ingesta(s3_bucket, s3_object_name, new_data)\n",
    "\n",
    "            # Actualizar la fecha de la última inspección en el archivo de estado\n",
    "            if 'inspection_date' in new_data.columns:\n",
    "                ultima_fecha = new_data['inspection_date'].max()\n",
    "                guardar_fecha_ultimo_proceso(ultima_fecha)\n",
    "        else:\n",
    "            print(\"No hay datos nuevos desde la última ingesta. No se generará un archivo.\")\n",
    "\n",
    "    else:\n",
    "        # Ingesta inicial\n",
    "        initial_data = ingesta_inicial(client)\n",
    "        initial_data.columns = initial_data.columns.str.strip().str.lower()\n",
    "\n",
    "        # Generar nombre del archivo S3 para la ingesta inicial\n",
    "        s3_object_name = f\"ingesta/inicial/inspecciones-historicas-{fecha_hoy}.pkl\"\n",
    "        guardar_ingesta(s3_bucket, s3_object_name, initial_data)\n",
    "\n",
    "        # Guardar la fecha de la última inspección en el archivo de estado\n",
    "        if 'inspection_date' in initial_data.columns:\n",
    "            ultima_fecha = initial_data['inspection_date'].max()\n",
    "            guardar_fecha_ultimo_proceso(ultima_fecha)\n",
    "\n",
    "# Ejecutar el proceso de ingesta en intervalos\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        ingest_data()\n",
    "        time.sleep(interval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este codigo se ejecuta dentro de un entorno virtual, dentro de un contenedor, dentro de una instancia EC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con las credenciales de AWS pueden ver los objetos del bucket"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
